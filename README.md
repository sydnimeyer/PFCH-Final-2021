# PFCH-Final-2021
A working project providing foundation for generating word clouds from JStor Data for Research Metadata

While financial agreements between private funders and higher education institutions is not a recent phenomenon, uncovered contracts between George Mason University and the Charles Koch Foundation revealed a uniquely invasive strategy to exert control over faculty hiring decisions. Between 1990-2011, the Koch Foundation funding for the Mercatus Center at George Mason required that the Koch Foundation could sway or outright make faculty hiring decisions, greatly shaping the faculty composition of this free market research think tank. (Flaherty, Colleen. “Uncovering Koch Role in Faculty Hires.” Inside Higher Ed. 1 May 2018. https://www.insidehighered.com/news/2018/05/01/koch-agreements-george-mason-gave-foundation-role-faculty-hiring-and-oversight) Though the ability to serve on Mercatus hiring committees appears to have been removed from recent contracts, student activism has unearthed similar language in Koch Foundation contracts with Florida State University and Utah State University.

This project is particularly interested in the potential effect on academic freedom these kinds of agreements foreclose, using keyword data from the Economics Department of George Mason University to track language shifts in published articles overtime. Using Jstor Data for Research (https://docs.constellate.org/differences-from-jstors-data-for-research-dfr/) metadata spanning 1990-2021, I began extracting citational information from the metadata documents about citation practices and keywords. The initial metadata request and extraction practice began in early April, prior to DfR transitioning to a new data platform that provides NGram extraction rather than full citational information (https://docs.constellate.org/topic/about/). This working project finds itself in that transitory space, attempting to pythonic recreation of NGram functionality.

With the provided metadata from JStor, I extracted the publication year and keywords listed for each article from the XML. I testing with the ArticlesKeywordYear. file before attempting a glob of all the journal articles in the metadata document. The full glob attempt is documented in the FailedGlobArticleYearKeyword file. Given the inconsistencies of the metadata, I manually selected 152 XML files of journal metadata to glob amd extract keyword information from, preparing a panda data frame for a word cloud. This produced the kwdglob_keyword CSV inluded in this repository. This dataframe was carried into the WordCloudCreation file, where. I used pandas functionality wtih regular expressions to clean the data before attempting to make a word cloud using Wordcloud and Matplotlib in python. Projected further work would require extensive data clean up with pandas.

This repository is more of a sandbox than a meaningfully finished project, meant to provide foundational tools for librarians and information professionals to understand how to replicate data services provided by licensed resources. Files provided leave breadcrumbs for myself other python learners to test their human NGram capacity with Python, at the very least to understand how to reproduce software capacities lest they transition to a new form mid project.


